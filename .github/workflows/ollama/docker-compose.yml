services:
  server:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    healthcheck:
      test:  [ "CMD-SHELL", "bash", "-c", "{ printf >&3 'GET / HTTP/1.0\\r\\n\\r\\n'; cat <&3; } 3<>/dev/tcp/localhost/11434 | grep 'Ollama is' || exit 1"  ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  client:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    - SYSTEM=${SYSTEM}
    - PROMPT=${PROMPT}
    entrypoint:
    - /bin/bash
    - -c
    - |
      set -euxo pipefail

      cat <<'EOF' > ./custom.modelfile
      FROM deepseek-v3:latest
      TEMPLATE "{{- range $i, $_ := .Messages }}
{{- if eq .Role "user" }}<｜User｜>
{{- else if eq .Role "assistant" }}<｜Assistant｜>
{{- end }}{{ .Content }}
{{- if eq (len (slice $.Messages $i)) 1 }}
{{- if eq .Role "user" }}<｜Assistant｜>
{{- end }}
{{- else if eq .Role "assistant" }}<｜end▁of▁sentence｜><｜begin▁of▁sentence｜>
{{- end }}
{{- end }}
      "
      PARAMETER stop <｜begin▁of▁sentence｜>
      PARAMETER stop <｜end▁of▁sentence｜>
      PARAMETER stop <｜User｜>
      PARAMETER stop <｜Assistant｜>
      EOF

      ollama create custom --file ./custom.modelfile
      ollama run custom "<|Assistant|>${SYSTEM}<|User|>${PROMPT}<|Assistant|>"

    depends_on:
      server:
        condition: service_healthy
