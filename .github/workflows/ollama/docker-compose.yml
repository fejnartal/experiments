services:
  server:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    healthcheck:
      test:  [ "CMD-SHELL", "bash", "-c", "{ printf >&3 'GET / HTTP/1.0\\r\\n\\r\\n'; cat <&3; } 3<>/dev/tcp/localhost/11434 | grep 'Ollama is' || exit 1"  ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  client:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    - SYSTEM=${SYSTEM}
    - PROMPT=${PROMPT}
    entrypoint:
    - /bin/bash
    - -c
    - |
      set -euxo pipefail

      cat <<'EOF' > ./custom.modelfile
      FROM llama2:latest
      TEMPLATE "[INST] <<SYS>>{{ .System }}<</SYS>>

      {{ .Prompt }} [/INST]
      "
      SYSTEM """${SYSTEM}"""
      PARAMETER stop [INST]
      PARAMETER stop [/INST]
      PARAMETER stop <<SYS>>
      PARAMETER stop <</SYS>>
      EOF

      ollama create custom --file ./custom.modelfile
      ollama run custom "${PROMPT}"

    depends_on:
      server:
        condition: service_healthy
