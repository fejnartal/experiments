services:
  server:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    healthcheck:
      test:  [ "CMD-SHELL", "bash", "-c", "{ printf >&3 'GET / HTTP/1.0\\r\\n\\r\\n'; cat <&3; } 3<>/dev/tcp/localhost/11434 | grep 'Ollama is' || exit 1"  ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  client:
    image: ollama/ollama
    environment:
    - OLLAMA_HOST=server
    - SYSTEM=${SYSTEM}
    - PROMPT=${PROMPT}
    entrypoint:
    - /bin/bash
    - -c
    - |
      set -euxo pipefail

      cat <<'EOF' > ./custom.modelfile
      FROM deepseek-v2:16b-lite-chat-q8_0
      TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
      {{- range $i, $_ := .Messages }}
      {{- $last := eq (len (slice $.Messages $i)) 1}}
      {{- if eq .Role "user" }}<｜User｜>{{ .Content }}
      {{- else if eq .Role "assistant" }}<｜Assistant｜>{{ .Content }}{{- if not $last }}<｜end▁of▁sentence｜>{{- end }}
      {{- end }}
      {{- if and $last (ne .Role "assistant") }}<｜Assistant｜>{{- end }}
      {{- end }}
      """
      PARAMETER stop <｜begin▁of▁sentence｜>
      PARAMETER stop <｜end▁of▁sentence｜>
      PARAMETER stop <| User |>
      PARAMETER stop <| Assistant |>
      EOF

      ollama create custom --file ./custom.modelfile
      ollama run custom "<| Assistant |>${SYSTEM}<| User |>${PROMPT}<| Assistant |>"

    depends_on:
      server:
        condition: service_healthy
